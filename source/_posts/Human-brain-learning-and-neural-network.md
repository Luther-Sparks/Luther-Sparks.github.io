---
title: Human brain learning and neural network
date: 2023-03-20 15:25:02
tags: 思考
comments: true
author: Luther
---

> This blog was written in 2021 to record my thoughts on AI technology at that time.

## Introduction
As the basis of deep learning, neural network has the momentum to dominate the world in the field of machine learning. The neural network is inspired by the structure of neurons in the human brain. The activation of neurons is used to simulate the electrical signal threshold of synapses, and the connection of neurons is used to simulate the information transmission of neurons in the human brain. But in fact, there is still a big difference between our neural network and the processing of information by the human brain. This blog will discuss some advantages and disadvantages of the existing neural network system from the perspective of human brain learning.

## Prior knowledge
We know that the neural network needs a lot of data and calculations as support to get a relatively good effect, and every time a new model is trained, the data needs to be recalculated and processed to relearn and understand the characteristics behind the data and nature. But our human brain is much better at this aspect. We don't need to re-understand what we have understood before, but only need to use what we have learned or what we have learned innately to make new attempts. This effect comes from Our ample prior knowledge. Prior knowledge intelligence is a very important indicator. People do not need to understand and learn, they can roughly perceive pain, fear, joy, etc., and they can innately perceive what may be dangerous. These are deeply imprinted The innate prior knowledge in our DNA, with the blessing of prior knowledge, we can learn things faster, and have a much better initial effect than neural networks. The existing neural network structure also has an imitation of similar situations, which is the well-known pre-training model. The pre-trained model provides the results of a trained model as prior knowledge, allowing artificial intelligence to learn and understand the information behind the data faster and easier. This sounds like a very reasonable and effective way, and in fact it does have a very good effect. For a long time, the pre-training + fine-tuning method has been a very convenient and effective model training method for a long time, and has even become the mainstream in academia and industry. For example, BERT, which has made countless rankings in the past two years, and the thriving huggingface-transformer, all show us the power of pre-training with actual effects.

Under such circumstances, we have almost accepted the idea that pre-training is the answer to artificial intelligence, but is this really reasonable? We know that our pre-training model is also a pre-designed artificial intelligence model, and often The neural network structure, and this pre-training model does not have prior knowledge, so the pre-training model does not belong to the artificial intelligence model that we think is an excellent simulation of the human brain, which falls into a dilemma of circular argument. Therefore, the pre-training model is logically difficult to be self-consistent, and it is difficult to prove that this is the most suitable simulation of the human brain.

I believes that in order for artificial intelligence to truly have extremely rich and effective prior knowledge like the human brain, it needs to use a large amount of data and models for pre-training, and the generated model is further iterated as a pre-training model, and It is best to use all effective models as pre-training models to truly aggregate and unify prior knowledge, so that the absence of prior knowledge in the pre-training model can be diluted with an effective understanding of the data. But at the same time, this requires more powerful computing power, more accurate data processing, and stronger model distributed management. These will be obstacles on the road to realizing artificial intelligence that truly simulates the human brain.

## Versatility
The general performance of the existing neural network is not good. On the one hand, it shows that the generalization ability is weak, and the performance may not be good when facing data other than the data set, which is the well-known phenomenon of over-fitting; Aspects show that when faced with different tasks, multiple models are often required to deal with these tasks. For example, classification tasks and regression tasks require different models to achieve good results.

In fact, there are many more mature solutions for the former aspect, such as introducing noise and regularization to bring an external disturbance, reducing the dependence on data; randomly discarding some results through dropout to simulate sudden environmental conditions, etc. wait. This series of operations allows the neural network to have stronger and stronger generalization capabilities, but now there is no set of general operations that allow all neural network models to avoid overfitting in the same way, making the model unable to resemble the human brain. Faced with various complex situations. On the latter aspect, there are also solutions that require multiple models to apply different tasks, that is, use a unified paradigm for model design. This design makes the designed models have similar structures and different functions, and then unify them A more general artificial intelligence model can be obtained by combining them. For example, the Pretrained+Prompt paradigm in the NLP field enables the model to have a preliminary understanding of information through the pretraining model. The analogy of the human brain is to have a general understanding of a class of things, and then improve the model by giving different prompts to different tasks. behavior, making it a more complete language model.

This strategy seems to be very beautiful, but the problem lies in the effectiveness of this strategy and its dependence on pre-trained models and hints. Today, we still have limited exploration in the field of artificial intelligence, and it is difficult to find a very good one in multiple fields. The general paradigm of the effect, such as breaking the barriers of NLP and CV, and implementing a model to process multiple types of data. This requires the introduction of multi-modality, the fusion and processing of various information and features, the efficient use of data and the promotion of models to further promote the realization of general artificial intelligence. Nowadays, the academic circle has also made certain achievements in this area, such as the XMC-GAN model that generates images from descriptive sentences or paragraphs, guides image processing through text, gradually breaks the barriers between different fields, and creates more general artificial intelligence. intelligent.

## Key feature extraction and processing
A key difference between the human brain and artificial intelligence is that the human brain can accept more external stimuli, acquire more information, and process it quickly. For example, when seeing a picture, the human brain will be immediately attracted by the brightly colored parts, or can quickly locate a certain key content. For computers, this picture will be processed into a grayscale image, at which point some information has been lost, and the cold numbers cannot allow the machine to quickly perceive the key information in a picture like humans.

There are two difficulties involved, one is the extraction of features, and the other is the processing of features. A picture and a piece of text are just a bunch of combined symbols for a computer. How to extract features from them to obtain the information behind the data has become a major problem for AI. In the field of NLP, the difficulty may be less. After all, most languages can be symbolized and standardized, such as verb phrases and noun phrases in English. We can roughly determine which word it belongs to by the distribution position of the word. Class words, so that their features can be further extracted. In the CV field, it is relatively more difficult. Images contain a lot of information, and the existing computing power cannot support processing so much information at one time. The convolution operation introduced by the academic circles solves this problem to a large extent. The convolution kernel is used to map part of the information to a lower dimension to reduce the pressure of data processing, and then the whole information can be obtained by integrating the information. Image features and information.

With this idea of analyzing the whole through the part, the convolutional neural network has achieved subversive achievements. But from the perspective of the human brain, this feature extraction method is contrary to logic. The human brain will first pay attention to the whole of a piece of information, such as whether the content of an image is a landscape, oil painting or sketch, etc., and whether a sentence is a serious announcement or a romantic poem, and then pay attention to the details. This method can quickly and effectively carry out In the processing of information, there is no need to waste computing power in some directions that have been eliminated. The neural network processes image information in the opposite direction, first analyzing the details and then integrating them into a whole, which brings interference of useless information and waste of computing power.

In terms of feature processing, the human brain also has an extremely sophisticated design. We will not process all the data received, but only focus on one part of the information processing, and selectively ignore the other parts. This information processing method effectively reduces the difficulty of data processing, allowing us to more Respond to a range of information faster and more accurately. Inspired by this, the academic community invented the attention mechanism, using the Query-Key-Value model to obtain the key parts of the information by calculating the attention weight of the information, so that the model can pay more attention to the key information to achieve better results.

## Summary
In this blog, I compared the similarities and differences between the information processing methods of the human brain and the existing neural network models, analyzed the advantages and disadvantages of the existing neural networks, obtained inspiration from the information processing methods of the human brain, and re-examines the value and advantages of existing technologies. Embarrassment. As a supporter of deep learning, the author hopes that the neural network can gain more inspiration from the human brain mechanism and achieve new breakthroughs.